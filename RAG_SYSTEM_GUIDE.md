# RAG对比系统使用指南

## 问题诊断

根据您提供的测试结果，系统存在以下主要问题：

### 1. 测试数据不匹配
- **问题**：您测试的是关于"易书元"、"阔南山"等小说内容，但系统默认的测试数据是AI技术相关文档
- **解决**：需要使用包含小说内容的文档进行测试

### 2. 向量检索器初始化问题
- **问题**：中文BGE模型需要HuggingFace token才能加载
- **解决**：设置环境变量或使用公开模型

### 3. 文档切块参数不合适
- **问题**：默认的切块参数（父块1000字符，子块200字符）可能不适合小说文本
- **解决**：使用优化的切块参数

## 解决方案

### 步骤1：环境配置

1. **设置HuggingFace Token**（推荐）：
```bash
export HUGGINGFACE_HUB_TOKEN="your_token_here"
# 或者
export HF_TOKEN="your_token_here"
```

2. **设置LLM配置**（可选）：
```bash
export LLM_BASE_URL="https://api.openai.com"
export LLM_API_KEY="your_api_key_here"
export LLM_MODEL="gpt-3.5-turbo"
```

### 步骤2：运行调试脚本

```bash
python debug_rag_system.py
```

这个脚本会：
- 测试向量检索器是否正常工作
- 测试注意力检索器是否正常工作
- 测试文档切块效果
- 生成测试查询列表

### 步骤3：使用优化配置

1. **启动服务器**：
```bash
python server.py
```

2. **访问Web界面**：
```
http://localhost:8000
```

3. **上传测试文档**：
   - 使用提供的 `test_novel.txt` 文件
   - 或者上传您自己的小说文本文件

4. **优化切块参数**：
   - 父块大小：800字符
   - 父块重叠：150字符
   - 子块大小：150字符
   - 子块重叠：30字符

### 步骤4：配置LLM

在Web界面中配置：
- **Base URL**：您的LLM服务地址
- **API Key**：您的API密钥
- **模型名**：如 gpt-3.5-turbo
- **RAG提示词**：使用优化的提示词模板

## 优化建议

### 1. 文档切块优化

对于小说文本，建议使用以下参数：
```python
{
    "parent_chunk_size": 800,      # 保持段落完整性
    "parent_overlap": 150,         # 避免信息丢失
    "sub_chunk_size": 150,         # 便于精确匹配
    "sub_overlap": 30,             # 保持上下文
}
```

### 2. 向量模型选择

优先级顺序：
1. `BAAI/bge-large-zh-v1.5`（需要token）
2. `BAAI/bge-base-zh-v1.5`（需要token）
3. `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`（公开模型）

### 3. 注意力机制优化

- 启用语义嵌入匹配
- 使用中文分词（jieba）
- 配置中文停用词
- 优化注意力权重

### 4. RAG提示词优化

使用更严格的提示词：
```
你是一个严谨的中文助手。请严格基于给定的参考上下文回答用户问题：

要求：
1. 如果上下文无法支持答案，必须直接回答："无法根据参考上下文回答。"
2. 禁止编造或引入上下文以外的信息。
3. 回答要准确、精炼、有逻辑。
4. 如果上下文中有多个相关信息，请综合回答。
5. 对于小说类文本，注意人物、地点、事件的准确性。
```

## 常见问题解决

### 1. 向量检索器加载失败

**错误信息**：`401 Unauthorized` 或 `模型加载失败`

**解决方案**：
- 检查HuggingFace token是否正确设置
- 尝试使用公开模型
- 检查网络连接

### 2. 注意力得分过低

**问题**：注意力RAG经常返回"无法根据参考上下文回答"

**解决方案**：
- 检查文档切块是否包含相关信息
- 调整注意力权重配置
- 启用语义嵌入匹配

### 3. LLM调用失败

**错误信息**：`LLM网络请求失败` 或 `LLM调用失败`

**解决方案**：
- 检查LLM服务是否可用
- 验证API key是否正确
- 检查网络连接

### 4. 检索结果不准确

**问题**：检索到的文档与问题不相关

**解决方案**：
- 调整文档切块参数
- 使用更合适的向量模型
- 优化相似度阈值

## 测试验证

### 1. 运行调试脚本
```bash
python debug_rag_system.py
```

### 2. 检查输出
确保以下组件正常工作：
- ✅ 向量检索器
- ✅ 注意力检索器
- ✅ LLM集成

### 3. 测试查询
使用提供的测试查询验证系统：
- "易书元是谁"
- "阔南山的山神可能是谁"
- "易书元在阔南山遇见了谁"

### 4. 预期结果
系统应该能够：
- 正确识别查询类型
- 检索到相关文档片段
- 生成准确的答案

## 性能优化

### 1. 缓存优化
- 启用文档缓存
- 启用嵌入缓存
- 设置合适的缓存过期时间

### 2. 批处理优化
- 使用批处理进行嵌入计算
- 优化注意力计算批次大小

### 3. 并发优化
- 设置合适的工作线程数
- 配置超时时间

## 监控和日志

### 1. 日志配置
- 设置日志级别为INFO
- 配置日志文件轮转
- 监控错误日志

### 2. 性能监控
- 监控检索时间
- 监控LLM调用延迟
- 监控内存使用

### 3. 质量评估
- 评估检索准确性
- 评估答案质量
- 对比两种RAG方法的效果

## 总结

通过以上优化，您的RAG对比系统应该能够：

1. **正确加载模型**：使用合适的中文向量模型
2. **准确切块文档**：保持小说文本的上下文完整性
3. **有效检索信息**：找到与查询相关的文档片段
4. **生成准确答案**：基于上下文生成合理的回答
5. **对比两种方法**：展示注意力RAG和向量RAG的差异

如果仍然遇到问题，请运行调试脚本并查看详细的错误信息。 